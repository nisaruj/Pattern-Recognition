{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pattern_HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "njeZi8LryEnm",
        "Q3rHTHwi6vFz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_GvrpIQxFBH",
        "colab_type": "text"
      },
      "source": [
        "# Hello Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtZOxjU0RbsT",
        "colab_type": "text"
      },
      "source": [
        "**K-mean clustering**\n",
        "\n",
        "\n",
        "Concretely, if we are given N data points, x1, x2, ..., xN , and we would like\n",
        "to form K clusters. We do the following;\n",
        "1. **Initialization**: Pick K random data points as K centroid locations c1,\n",
        "c2, ..., cK.\n",
        "2. **Assign**: For each data point k, find the closest centroid. Assign that\n",
        "data point to the centroid. The distance used is typically Euclidean distance.\n",
        "3. **Update**: For each centroid, calculate the mean from the data points\n",
        "assigned to it.\n",
        "4. **Repeat**: repeat step 2 and 3 until the centroids stop changing (convergence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw83TGqSxksX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "points = [(1, 2), (3, 3), (2, 2), (8, 8), (6, 6),\n",
        "            (7, 7), (-3, -3), (-2, -4), (-7, -7)]\n",
        "\n",
        "def plot(clusters, title):\n",
        "  DOT_COLORS = 'rgbcmyk'\n",
        "  fig = plt.figure()\n",
        "  plt.title(title)\n",
        "  for i in range(len(clusters)):\n",
        "    for point in clusters[i]:\n",
        "      plt.plot(point[0], point[1], 'ro', c=DOT_COLORS[i])\n",
        "  fig.show()\n",
        "\n",
        "def dist(p1, p2):\n",
        "    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n",
        "\n",
        "def print_point(p):\n",
        "    print('(', p[0], ',', p[1], ')', end=' ')\n",
        "  \n",
        "def k_mean(start_centroids, ITER_COUNT = 4, debug = False):\n",
        "  centroids = start_centroids\n",
        "  k = len(centroids)\n",
        "\n",
        "  def print_assign(section):\n",
        "      print('Assign Step')\n",
        "      for i in range(k):\n",
        "          print('Cluster', i, ':')\n",
        "          for point in section[i]:\n",
        "              print_point(point)\n",
        "          print()\n",
        "      print()\n",
        "\n",
        "  def update_centroids(section):\n",
        "      new_centroid = []\n",
        "      for i in range(k):\n",
        "          x = [pt[0] for pt in section[i]]\n",
        "          y = [pt[1] for pt in section[i]]\n",
        "          new_centroid.append((sum(x)/len(x), sum(y)/len(y)))\n",
        "      return new_centroid\n",
        "\n",
        "\n",
        "  def print_centroids(centroids):\n",
        "      print(\"Centroids: \", end='' )\n",
        "      for c in centroids:\n",
        "        print_point(c)\n",
        "      print()\n",
        "\n",
        "  for i in range(ITER_COUNT):\n",
        "      if debug: \n",
        "        print(\"Iter\", i)\n",
        "        print_centroids(centroids)\n",
        "      section = []\n",
        "      for j in range(k):\n",
        "          section.append([])\n",
        "      for point in points:\n",
        "          section[min([(dist(point, centroids[i]), i) for i in range(k)])[1]].append(point)\n",
        "      if debug:\n",
        "        print_assign(section)\n",
        "      centroids = update_centroids(section)\n",
        "  \n",
        "  return { \"centroids\": centroids, \"clusters\": section }\n",
        "\n",
        "# T4\n",
        "T4 = k_mean([(3, 3), (2, 2), (-3, -3)])\n",
        "# T5\n",
        "T5 = k_mean([(-3, -3), (2, 2), (-7, -7)])\n",
        "\n",
        "plot(T4['clusters'], 'T4')\n",
        "plot(T5['clusters'], 'T5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGSjXznQSlNa",
        "colab_type": "text"
      },
      "source": [
        "We can use explained variance to measure clustering quality.\n",
        "\n",
        "explained variance = between-cluster var / all-data var\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aso7nKIsa9I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def all_point_centroid():\n",
        "  x = [pt[0] for pt in points]\n",
        "  y = [pt[1] for pt in points]\n",
        "  return (sum(x)/len(x), sum(y)/len(y))\n",
        "\n",
        "all_centroid = all_point_centroid()\n",
        "\n",
        "def all_data_variance():\n",
        "  s = 0\n",
        "  for pt in points:\n",
        "    s += dist(pt, all_centroid) ** 2\n",
        "  return s / (len(points) - 1)\n",
        "    \n",
        "def between_cluster_variance(centroids, clusters):\n",
        "  s = 0\n",
        "  for i in range(len(centroids)):\n",
        "    s += len(clusters[i]) * dist(centroids[i], all_centroid) ** 2\n",
        "  return s / (len(points) - 1)\n",
        "\n",
        "def explained_var(k_mean_result): # { 'centroids': [(x, y)], 'clusters': [[(x, y)]] }\n",
        "  return between_cluster_variance(k_mean_result['centroids'], k_mean_result['clusters']) / all_data_variance()\n",
        "  \n",
        "# T6\n",
        "T4_res = k_mean([(3, 3), (2, 2), (-3, -3)])\n",
        "T5_res = k_mean([(-3, -3), (2, 2), (-7, -7)])\n",
        "\n",
        "print(explained_var(T4_res)) # 0.9298618490967058\n",
        "print(explained_var(T5_res)) # 0.8138947927736452"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0geUN1BToXX",
        "colab_type": "text"
      },
      "source": [
        "The result tells us that T4 is better than T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4StUYgUKt6",
        "colab_type": "text"
      },
      "source": [
        "To determine the best K for this question, we can use Elbow method though it isn't so accurate.\n",
        "Elbow method chooses K where increasing complexity doesn't yield much in return. (ie. minimal K that explains at least 95% of the all-data variance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yABPjPXVcyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OT1\n",
        "import matplotlib.pyplot as plt\n",
        "from random import sample\n",
        "import numpy as np\n",
        "\n",
        "MAX_K = 10\n",
        "ITER_PER_K = 50\n",
        "\n",
        "# Plotting\n",
        "exp_vars = []\n",
        "num_of_cluster = [k for k in range(1, MAX_K)]\n",
        "fig = plt.figure()\n",
        "\n",
        "for k in num_of_cluster:\n",
        "  s = 0\n",
        "  tries = 0\n",
        "  for i in range(ITER_PER_K):\n",
        "    try:\n",
        "      start_pts = sample(points, k)\n",
        "      s += explained_var(k_mean(start_pts, 100))\n",
        "      tries += 1\n",
        "    except:\n",
        "      pass\n",
        "  exp_vars.append(s / tries)\n",
        "\n",
        "print(exp_vars)\n",
        "\n",
        "plt.plot(num_of_cluster, exp_vars, marker='o')\n",
        "line_95p = np.linspace(0, MAX_K, 1000)\n",
        "plt.plot(line_95p, np.zeros(1000) + 0.95) # 95% line\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Explained Variance')\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZR2LaBapXP5",
        "colab_type": "text"
      },
      "source": [
        "From the result above, K=4 is the best K for this set of points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njeZi8LryEnm",
        "colab_type": "text"
      },
      "source": [
        "# My Heart Will Go On"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX5VGHWrKbQ7",
        "colab_type": "text"
      },
      "source": [
        "**Load data set and test set** and fill all NaN value with median (or mode) before training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKRX2xIeKISo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
        "train = pd.read_csv(train_url) #training set\n",
        "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
        "test = pd.read_csv(test_url) #test set\n",
        "\n",
        "# T7: Find Median\n",
        "print('Median =', train['Age'].median()) # 28.0\n",
        "train['Age'] = train['Age'].fillna(train['Age'].median())\n",
        "\n",
        "# Assign embarked value to number\n",
        "MAPPING_CONST = { 'Embarked': { 'S': 0, 'C': 1, 'Q': 2 }, 'Sex': { 'male': 0, 'female': 1 } }\n",
        "for key in MAPPING_CONST:\n",
        "  for val in MAPPING_CONST[key]:\n",
        "    train.loc[train[key] == val, key] = MAPPING_CONST[key][val]\n",
        "    test.loc[test[key] == val, key] = MAPPING_CONST[key][val]\n",
        "\n",
        "# T8: Find Mode\n",
        "print('Embarked Mode =', train.mode()['Embarked'][0]) # 0 (S)\n",
        "train['Embarked'] = train['Embarked'].fillna(train.mode()['Embarked'][0])\n",
        "print('Sex Mode =', train.mode()['Sex'][0]) # 0 (male)\n",
        "train['Sex'] = train['Sex'].fillna(train.mode()['Sex'][0])\n",
        "\n",
        "# Fill NaN for test data\n",
        "test['Embarked'] = test['Embarked'].fillna(test.mode()['Embarked'][0])\n",
        "test['Sex'] = test['Sex'].fillna(test.mode()['Sex'][0])\n",
        "test['Age'] = test['Age'].fillna(test['Age'].median())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIz_Ugr-6VMB",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFMjlgD6yJqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# T9 Logistic Regression\n",
        "features = [\"Pclass\",\"Sex\",\"Age\",\"Embarked\"]\n",
        "\n",
        "# Initiate plot of loss function\n",
        "fig = plt.figure()\n",
        "loss = []\n",
        "iter_count = []\n",
        "PLOT_FILE_NAME = 'loss.png'\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "def sigmoid(theta, x_i):\n",
        "  dot = np.dot(theta, x_i)\n",
        "  return 1 / (1 + math.exp(-dot))\n",
        "\n",
        "def updated_theta(theta, data, result, rate):\n",
        "  new_theta = np.array(theta)\n",
        "  for j in range(len(features) + 1):\n",
        "    s = 0\n",
        "    for i in range(data.shape[0]):\n",
        "      s += (result[i] - sigmoid(np.array([theta]), np.array([data[i]]).T)) * data[i][j]\n",
        "    new_theta[j] = theta[j] + rate * s\n",
        "  return new_theta\n",
        "\n",
        "def logistic_regression(rate = 0.00001, ITER_COUNT = -1):\n",
        "  raw_data = np.array(train[features].values, dtype=int)\n",
        "  data = np.append(np.ones((raw_data.shape[0], 1)), raw_data, axis = 1)\n",
        "  result = np.array(train['Survived'].values, dtype=int)\n",
        "  theta = np.zeros(len(features) + 1)\n",
        "  i = 0\n",
        "  while i < ITER_COUNT or ITER_COUNT == -1:\n",
        "    theta = updated_theta(theta, data, result, rate)\n",
        "    \n",
        "    # Logging every 100 iter\n",
        "    if i % 100 == 0: \n",
        "      print('Iter', i, ':', theta)\n",
        "      loss.append(loss_function(theta, data, result))\n",
        "      iter_count.append(i)\n",
        "      plt.plot(iter_count, loss)\n",
        "      fig.savefig(PLOT_FILE_NAME)\n",
        "      \n",
        "    i += 1\n",
        "  return theta\n",
        "\n",
        "def loss_function(theta, data, result):\n",
        "  s = 0\n",
        "  for i in range(data.shape[0]):\n",
        "    hyp = sigmoid(np.array([theta]), np.array([data[i]]).T)\n",
        "    s -= result[i] * math.log(hyp) + (1 - result[i]) * math.log(1 - hyp)\n",
        "  return s / data.shape[0]\n",
        "  \n",
        "model = logistic_regression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-omz-646Zfq",
        "colab_type": "text"
      },
      "source": [
        "Write the solution in .csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHybXjZ7IUj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_FILE_NAME = 'solution.csv'\n",
        "\n",
        "def test_model(theta, THRESHOLD = 0.5):\n",
        "  # Result DataFrame\n",
        "  result = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
        "  \n",
        "  raw_data = np.array(test[features].values, dtype=int)\n",
        "  data = np.append(np.ones((raw_data.shape[0], 1)), raw_data, axis = 1)\n",
        "  for i in range(data.shape[0]):\n",
        "    train_sol = sigmoid(np.array([theta]), np.array([data[i]]).T)\n",
        "    result.loc[i] = [test['PassengerId'][i]] + [int(train_sol >= THRESHOLD)]\n",
        "  \n",
        "  return result\n",
        "\n",
        "model = np.array([2.07070249,-1.19638948,2.57579964,-0.03372561,0.32077026])\n",
        "result = test_model(model)\n",
        "result.to_csv(OUTPUT_FILE_NAME, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hodJjcjabqu9",
        "colab_type": "text"
      },
      "source": [
        "Try adding some higher order features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfQpCs1nbyNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OT4\n",
        "high_order_features = [\"Pclass\"]\n",
        "features = [\"Pclass\",\"Sex\",\"Age\",\"Embarked\"]\n",
        "\n",
        "for feature in high_order_features:\n",
        "  new_fname = feature + \"2\"\n",
        "  train[new_fname] = train[feature] ** 2\n",
        "  test[new_fname] = test[feature] ** 2\n",
        "  features.append(new_fname)\n",
        "\n",
        "OUTPUT_FILE_NAME = 'solution_higher_order.csv'\n",
        "\n",
        "print(features)\n",
        "model = logistic_regression(0.000001)\n",
        "# [ 0.66371864  0.3197872   2.55857657 -0.03117537  0.37376845 -0.36377816]\n",
        "result = test_model(model)\n",
        "result.to_csv(OUTPUT_FILE_NAME, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntNSOyus3geE",
        "colab_type": "text"
      },
      "source": [
        "Try using only Sex and Age features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v9aYCuo3kYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OT5\n",
        "features = [\"Sex\", \"Age\"]\n",
        "OUTPUT_FILE_NAME = 'solution_sex_age.csv'\n",
        "\n",
        "# print(features)\n",
        "# model = logistic_regression(0.000001)\n",
        "model = np.array([-1.29902669,2.49762116,-0.00513416])\n",
        "result = test_model(model)\n",
        "result.to_csv(OUTPUT_FILE_NAME, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rHTHwi6vFz",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression\n",
        "Redo by using linear regression (Gradient descent) instread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6amLWMd61na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OT2\n",
        "def updated_theta_linear(theta, data, result, rate):\n",
        "  new_theta = np.array(theta)\n",
        "  for j in range(len(features) + 1):\n",
        "    s = 0\n",
        "    for i in range(data.shape[0]):\n",
        "      s += (result[i] - np.dot(np.array([theta]), np.array([data[i]]).T)) * data[i][j]\n",
        "    new_theta[j] = theta[j] + rate * s\n",
        "  return new_theta\n",
        "\n",
        "def linear_regression(rate = 0.0000001, ITER_COUNT = -1):\n",
        "  raw_data = np.array(train[features].values, dtype=int)\n",
        "  data = np.append(np.ones((raw_data.shape[0], 1)), raw_data, axis = 1)\n",
        "  result = np.array(train['Survived'].values, dtype=int)\n",
        "  theta = np.zeros(len(features) + 1)\n",
        "  i = 0\n",
        "  while i < ITER_COUNT or ITER_COUNT == -1:\n",
        "    theta = updated_theta_linear(theta, data, result, rate)\n",
        "    \n",
        "    # Logging every 100 iter\n",
        "    if i % 100 == 0: \n",
        "      print('Iter', i, ':', theta)\n",
        "      \n",
        "    i += 1\n",
        "  return theta\n",
        "\n",
        "model = linear_regression()\n",
        "# Iter 375800 : [ 0.60398843 -0.14915006  0.52049079 -0.00288894  0.05109822]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZtl-ZuG9HAP",
        "colab_type": "text"
      },
      "source": [
        "Write linear regression solution in .csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkF30WxM9Lem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_FILE_NAME = 'linear_solution.csv'\n",
        "def test_model_linear(theta, THRESHOLD = 0.5):\n",
        "  # Result DataFrame\n",
        "  result = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
        "  \n",
        "  raw_data = np.array(test[features].values, dtype=int)\n",
        "  data = np.append(np.ones((raw_data.shape[0], 1)), raw_data, axis = 1)\n",
        "  for i in range(data.shape[0]):\n",
        "    train_sol = np.dot(np.array([theta]), np.array([data[i]]).T)\n",
        "    result.loc[i] = [test['PassengerId'][i]] + [int(train_sol >= THRESHOLD)]\n",
        "  \n",
        "  return result\n",
        "\n",
        "model = np.array([0.60398843,-0.14915006,0.52049079,-0.00288894,0.05109822])\n",
        "result = test_model_linear(model)\n",
        "result.to_csv(OUTPUT_FILE_NAME, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMfXciewIqrs",
        "colab_type": "text"
      },
      "source": [
        "Redo by using linear regression (matrix inversion)\n",
        "\n",
        "theta = (X^TX)^-1X^TY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKJGXFfI9VI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OT3\n",
        "from numpy.linalg import inv\n",
        "\n",
        "def solve_linear():\n",
        "  raw_data = np.array(train[features].values, dtype=int)\n",
        "  data = np.append(np.ones((raw_data.shape[0], 1)), raw_data, axis = 1)\n",
        "  result = np.array(train['Survived'].values, dtype=int)\n",
        "  return inv(np.dot(data.T, data)).dot(data.T).dot(result)\n",
        "\n",
        "print(solve_linear())\n",
        "# [ 0.776742   -0.18848969  0.4908994  -0.00505977  0.04907325]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcO-eJv5MuoI",
        "colab_type": "text"
      },
      "source": [
        "We found that the weights from two method are similar.\n",
        "\n",
        "We'll report MSE of the difference between two weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqcLBxvQNAkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_gradient =  [0.60398843,-0.14915006,0.52049079,-0.00288894,0.05109822]\n",
        "weight_inverse = [0.776742,-0.18848969,0.4908994,-0.00505977,0.04907325]\n",
        "\n",
        "def MSE(a, b):\n",
        "  return sum([(a[i] - b[i]) ** 2 for i in range(len(a))]) / len(a)\n",
        "\n",
        "print(MSE(weight_gradient,weight_inverse))\n",
        "# 0.006455173160960742"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}